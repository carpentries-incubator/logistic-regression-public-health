---
source: Rmd
title: "An introduction to binary response variables"
objectives:
  - "Calculate the probabilities of success and failure given binary data."
  - "Interpret the expectation of a binary variable."
  - "Calculate and interpret the odds of success given binary data."
  - "Calculate and interpret the log odds of success given binary data."
keypoints:
questions:
teaching: 10
execises: 10
---
```{r, include=FALSE}
source("../bin/chunk-options.R")
# source("../bin/obtain_data.R")
load("../bin/data.RData")
knitr_fig_path("01-")
library(dplyr)
library(tidyr)
library(ggplot2)
library(latex2exp)
```

In this lesson we will work with binary outcome variables. 
That is, variables which can take on two values.
These could be $0$ vs. $1$, "success" vs. "failure" or "yes" vs. "no".

## Probabilities and expectation

By analysing binary data, we can estimate the probabilities of success and failure.
For example, we may be interested in the probability that individuals between the 
ages of 55 and 66, who have once been smokers, are still smoking during the NHANES
study. 

The probability of success is estimated by the proportion of individuals who are
still smoking. Similarly, the probability of failure is estimated by the 
proportion of individuals who are no longer smoking.

We calculate these values in RStudio by removing empty rows with `drop_na()`,
subsetting individuals of the appropriate age using `filter()`, counting
the number of individuals in each of the two levels of `SmokeNow` using
`count()` and calculating proportions by dividing the counts by the total number
of non-NA observations using `mutate()`. 


```{r calculate probabilities SmokeNow_Age}
dat %>%
  drop_na(SmokeNow) %>%
  filter(between(Age, 55, 66)) %>%
  count(SmokeNow, name = "n") %>%
  mutate(prop = n/sum(n))
```

We see that the probability of success is estimated as $0.38$ and the probability
of failure is estimated as $0.62$. In mathematical notation: 
$\text{Pr}(\text{SmokeNow} = \text{Yes}) = 0.38$ and $\text{Pr}(\text{SmokeNow} = \text{No}) = 0.62$

In the linear regression lessons, we modelled the *expectation* of the outcome 
variable, $E(y)$. In the case of binary variables, we will also work with the
expectation of the outcome variable. When $y$ is a binary variable, $E(y)$ is
equal to the probability of success. In our example above, $E(y) = \text{Pr}(\text{SmokeNow} = \text{Yes}) = 0.38$. 

> ## Exercise  
> You have been asked to study physical activity (`PhysActive`) 
> in individuals with an FEV1 (`FEV1`) between 3750 and 4250 in the NHANES data.   
> A) Estimate the probabilitites that someone is or is not physically active
> for individuals with an FEV1 between 3750 and 4250.   
> B) What value is $E(\text{PhysActive})$ for individuals 
> with an FEV1 between 3750 and 4250?
>
> > ## Solution
> > A) To obtain the probabilities:
> > 
> > ```{r probabilities PhysActive_FEV1}
> > dat %>%
> >   drop_na(PhysActive) %>%
> >   filter(between(FEV1, 3750, 4250)) %>%
> >   count(PhysActive) %>%
> >   mutate(prop = n/sum(n)) 
> > ```
> > 
> > We therefore estimate the probability of physical activity to be $0.68$
> > and the probability of no physical activity to be $0.32$.  
> > 
> > B) $E(\text{PhysActive}) = \text{Pr}(\text{PhysActive} = \text{Yes}) = 0.68$
> {: .solution}
{: .challenge}

> ## Why does $E(y)$ equal the probability of success?
> In general, the expectation of a variable equals its probability-weighted mean. 
> This is calculated by taking the sum of all values that a variable can take on, 
> each multiplied by the probability of that value occuring. 
> 
> In mathematical notation, this is indicated by:
> 
> $$E(y) = \sum_i\Big(y_i \times \text{Pr}(y = y_i)\Big)$$
> 
> In the case of a binary variable, the variable can take on 
> one of two values: $0$ and $1$. Therefore, the expectation becomes:
> 
> $$E(y) = \sum_i\Big(y_i \times \text{Pr}(y = y_i)\Big) = 0 \times \text{Pr}(y = 0) + 1 \times \text{Pr}(y = 1) = \text{Pr}(y = 1)$$
> 
> Since "success" is considered $y=1$, the expectation of a binary variable 
> equals the probability of success.
> 
{: .callout}

## Odds and log odds

Besides probabilities, binary data is often interpreted through odds. 
The odds are defined as:

$$\frac{E(y)}{1-E(y)}.$$

Since the expectation of $y$ equals the probability of success, the odds can also
be written as:

$$\frac{E(y)}{1-E(y)} = \frac{\text{Pr}(\text{Success})}{1-\text{Pr}(\text{Success})} = \frac{\text{Pr}(\text{Success})}{\text{Pr}(\text{Failure})}.$$

Therefore, an odds greater than $1$ indicates that the probability of success
is greater than the probability of failure. For example, an odds of 1.5 indicates that success is 1.5 times as likely as failure. An odds less than $1$ indicates that
the probability of failure is greater than the probability of success. For example, an odds of 0.75 indicates that success is 0.75 times as likely as failure. 

As we will see in the next episode, binary outcome variables can be modelled through
the *log odds*, i.e.:

$$\text{log}\left(\frac{E(y)}{1-E(y)}\right)$$

As we can see in the plot below, a log odds greater than zero is associated with 
a probability of success greater than 0.5. Likewise, a log odds smaller than 0 
is associated with a probability of success less than 0.5. 

```{r plot log odds vs expectation, echo = FALSE}
tibble(E_y = seq(0,1,length.out = 10000)) %>%
  mutate(odds = E_y / (1 - E_y),
         log_odds = log(E_y / (1 - E_y))) %>%
  ggplot(aes(x = E_y, y = log_odds)) +
  geom_line() +
  xlab(TeX(r'($E(y)$)')) +
  ylab(TeX(r'($log\left(\frac{E(y)}{1-E(y)}\right)$)')) +
  ylim(-6, 6)
```

The interpretation of the probabilities, odds and log odds is summarised in the
table below:

| Measure     | Turning point | Interpretation                                      |
|-------------|---------------|-----------------------------------------------------|
| Probability | 0.5           | Proportion of observations that are successes       |
| Odds        | 1.0           | How many times more likely is success than failure? |
| Log odds    | 0             | If log odds > 0, probability is > 0.5.              |

The odds and the log odds can be calculated in RStudio through an
extension of the code that we used to calculate the probabilities. 
From our table of probabilities we isolate the row with the probability
of success using `filter()`. We then calculate the odds and the log odds
using the `summarise()` function. 

```{r odds and log odds SmokeNow_Age}
dat %>%
  drop_na(SmokeNow) %>%
  filter(between(Age, 55, 66)) %>%
  count(SmokeNow) %>%
  mutate(prop = n/sum(n)) %>%
  filter(SmokeNow == "Yes") %>%
  summarise(odds = prop/(1-prop),
    log_odds = log(prop/(1-prop)))
```

> ## Exercise  
> You have been asked to study physical activity (`PhysActive`) 
> in individuals with an FEV1 (`FEV1`) between 3750 and 4250 in the NHANES data. 
> Calculate the odds and the log odds of physical activity
> for individuals with an FEV1 between 3750 and 4250. 
> How is the odds interpreted here?
>
> > ## Solution
> > 
> > ```{r odds and log odds PhysActive_FEV1}
> > dat %>%
> >   drop_na(PhysActive) %>%
> >   filter(between(FEV1, 3750, 4250)) %>%
> >   count(PhysActive) %>%
> >   mutate(prop = n/sum(n)) %>%
> >   filter(PhysActive == "Yes") %>%
> >   summarise(odds = prop/(1-prop),
> >     log_odds = log(prop/(1-prop)))
> > ```
> > 
> > Since the odds equal 2.17, we expect individuals with an FEV1 between 3750
> > and 4250 to be 2.17 times more likely to be physically active than not. 
> {: .solution}
{: .challenge}


> ## What does $\text{log}()$ do?
> The $\text{log}()$ is a transformation used widely in statistics, including
> in the modelling of binary variables. In general, $\text{log}_a(b)$ tells us
> to what power we need to raise $a$ to obtain the value $b$.  
> 
> For example, $2^3 = 2 \times 2 \times 2 = 8$. Therefore, $\text{log}_2(8)=3$, 
> since we raise $2$ to the power of $3$ to obtain 8.
> 
> Similarly, $\text{log}_3(81)=4$, since $3^4=81$. 
> 
> In logistic regression, we use $\text{log}_{e}()$, where $e$ is a mathematical 
> constant. The constant $e$ approximately equals 2.718. 
> 
> Rather than writing $\text{log}_{e}()$, 
> we write $\text{log}()$ for simplicity.
> 
> In R, we can calculate the log using the `log()` function. For example, 
> to calculate to what power we need to raise $e$ to obtain $10$:
> 
> ```{r log example}
> log(10)
> ```
> 
{: .callout}
